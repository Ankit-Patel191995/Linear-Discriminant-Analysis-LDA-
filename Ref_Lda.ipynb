{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95525511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\AKASH\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd7fa9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from nltk.tokenize import word_tokenize\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "rev = pd.read_csv(r\"Reviews.csv\")\n",
    "print(rev.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7ebe6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(headline):\n",
    "    le=WordNetLemmatizer()\n",
    "    word_tokens=word_tokenize(headline)\n",
    "    tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
    "    cleaned_text=\" \".join(tokens)\n",
    "    return cleaned_text\n",
    "rev['cleaned_text']=rev['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b285f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
    "vect_text=vect.fit_transform(rev['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ff00a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=10,random_state=42) \n",
    "lda_top=lda_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "616e7cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: \n",
      "Topic  0 :  2.1992339077819922 %\n",
      "Topic  1 :  2.1985043545548097 %\n",
      "Topic  2 :  17.024779111056574 %\n",
      "Topic  3 :  2.1983783888013466 %\n",
      "Topic  4 :  2.198732256871717 %\n",
      "Topic  5 :  65.38648016641734 %\n",
      "Topic  6 :  2.1984493293481044 %\n",
      "Topic  7 :  2.198234371396265 %\n",
      "Topic  8 :  2.1987634148807573 %\n",
      "Topic  9 :  2.1984446988910835 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 0: \")\n",
    "for i,topic in enumerate(lda_top[0]):\n",
    "    print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ac592a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "chip n\n",
      "snack n\n",
      "chocolate n\n",
      "cooky n\n",
      "taste n\n",
      "like n\n",
      "peanut n\n",
      "butter n\n",
      "great n\n",
      "good n\n",
      "Topic 1: \n",
      "shipping n\n",
      "salt n\n",
      "arrived n\n",
      "order n\n",
      "price n\n",
      "delivered n\n",
      "candy n\n",
      "good n\n",
      "fast n\n",
      "grey n\n",
      "Topic 2: \n",
      "product n\n",
      "like n\n",
      "would n\n",
      "taste n\n",
      "good n\n",
      "really n\n",
      "coconut n\n",
      "time n\n",
      "review n\n",
      "little n\n",
      "Topic 3: \n",
      "treat n\n",
      "dog n\n",
      "love n\n",
      "chew n\n",
      "training n\n",
      "ball n\n",
      "puppy n\n",
      "bone n\n",
      "baby n\n",
      "soft n\n",
      "Topic 4: \n",
      "popcorn n\n",
      "taste n\n",
      "drink n\n",
      "flavor n\n",
      "like n\n",
      "water n\n",
      "sugar n\n",
      "sweet n\n",
      "fruit n\n",
      "really n\n",
      "Topic 5: \n",
      "food n\n",
      "product n\n",
      "love n\n",
      "cat n\n",
      "month n\n",
      "like n\n",
      "year n\n",
      "time n\n",
      "hair n\n",
      "good n\n",
      "Topic 6: \n",
      "sauce n\n",
      "soup n\n",
      "spicy n\n",
      "pasta n\n",
      "great n\n",
      "cheese n\n",
      "flavor n\n",
      "chicken n\n",
      "make n\n",
      "rice n\n",
      "Topic 7: \n",
      "save n\n",
      "com n\n",
      "www n\n",
      "http n\n",
      "gp n\n",
      "subscribe n\n",
      "href n\n",
      "amazon n\n",
      "pack n\n",
      "ordering n\n",
      "Topic 8: \n",
      "store n\n",
      "amazon n\n",
      "find n\n",
      "price n\n",
      "local n\n",
      "grocery n\n",
      "order n\n",
      "espresso n\n",
      "great n\n",
      "product n\n",
      "Topic 9: \n",
      "coffee n\n",
      "flavor n\n",
      "like n\n",
      "taste n\n",
      "strong n\n",
      "good n\n",
      "cup n\n",
      "green n\n",
      "blend n\n",
      "bold n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKASH\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vocab = vect.get_feature_names()\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "        print(\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66920590",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic 0: \n",
    "chip n\n",
    "snack n\n",
    "chocolate n\n",
    "cooky n\n",
    "taste n\n",
    "like n\n",
    "peanut n\n",
    "butter n\n",
    "great n\n",
    "good n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic 1: \n",
    "shipping n\n",
    "salt n\n",
    "arrived n\n",
    "order n\n",
    "price n\n",
    "delivered n\n",
    "candy n\n",
    "good n\n",
    "fast n\n",
    "grey n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074359e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic 2: \n",
    "product n\n",
    "like n\n",
    "would n\n",
    "taste n\n",
    "good n\n",
    "really n\n",
    "coconut n\n",
    "time n\n",
    "review n\n",
    "little n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
